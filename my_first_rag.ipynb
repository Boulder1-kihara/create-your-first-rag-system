{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Boulder1-kihara/create-your-first-rag-system/blob/main/my_first_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval Augmented Generation\n",
        "**1. Retrieval (The Search):** You ask a question. The system instantly searches through your folder of PDFs and finds the exact page in your handout.\n",
        "\n",
        "**2. Augmented (The Context):** It takes that specific page and pastes it invisibly into the prompt.\n",
        "\n",
        "**3. Generation (The Answer):** The AI reads that specific handout and writes an answer based only on that source.\n",
        "\n",
        "\n",
        "# **Why this is great:**\n",
        "**No Hallucinations:** It won't make up fake facts; it sticks to your documents.\n",
        "\n",
        "**Privacy:** Since you want to run this locally on your HP laptop, your data stays with you.\n",
        "\n",
        "**Custom:** It becomes an expert on your specific degree, not just general knowledge."
      ],
      "metadata": {
        "id": "Uop3uakF3Qg9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to use the following libraries:\n",
        "1. **pypdf:** To read your PDF files.\n",
        "\n",
        "2. **langchain:** The framework that connects everything.\n",
        "\n",
        "3. **chromadb:** A database to store the \"memory\" (vectors).\n",
        "\n",
        "4. **sentence-transformers:** To turn your text into numbers the AI understands.\n",
        "\n",
        "Before everything it is advisable to use google colab and before writing any code run the following command in the terminal\n",
        "\n",
        "        pip install langchain langchain-community pypdf chromadb sentence-transformers"
      ],
      "metadata": {
        "id": "Rdp16R9l4_XB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIDHMrPqHbBQ",
        "outputId": "1662988d-8f20-4aa6-8b82-57b031de8605"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìñ Attempting to load: An Introduction to Machine Learning -- Miroslav KubaÃÅt -- ( WeLib.org ).pdf...\n",
            "‚úÖ SUCCESS! I read 348 pages.\n",
            "\n",
            "--- Here is what the first 500 characters look like: ---\n",
            "Miroslav/uni00A0Kubat\n",
            "An Introduction \n",
            "to Machine \n",
            "Learning\n",
            " Second Edition\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "#use the actual name of your pdf\n",
        "pdf_filename = \"An Introduction to Machine Learning -- Miroslav KubaÃÅt -- ( WeLib.org ).pdf\"\n",
        "\n",
        "try:\n",
        "    print(f\"üìñ Attempting to load: {pdf_filename}...\")\n",
        "\n",
        "    # This tool reads the PDF\n",
        "    loader = PyPDFLoader(pdf_filename)\n",
        "    pages = loader.load()\n",
        "\n",
        "    print(f\"‚úÖ SUCCESS! I read {len(pages)} pages.\")\n",
        "    print(\"\\n--- Here is what the first 500 characters look like: ---\")\n",
        "    print(pages[0].page_content[:500])\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {e}\")\n",
        "    print(\"Tip: Check if the file name is spelled exactly right!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMYS5WtGOxrT",
        "outputId": "0caed37a-8053-48cb-8884-54f074b38b6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Step 2: The \"Chop\" (Text Splitting)**\n",
        "**Here is the problem:** We can't feed all 348 pages into an AI at once. It's like trying to eat a whole pizza in one bite‚Äîyou'll choke (or run out of memory).\n",
        "\n",
        "We need to slice the book into smaller, bite-sized pieces called Chunks.\n",
        "\n",
        "\n",
        "#Why we do this:\n",
        "\n",
        "1. **Accuracy:** When you ask a question later, the AI can find the exact specific paragraph that has the answer, rather than scanning the whole book.\n",
        "\n",
        "2. **Overlap:** We keep a little bit of repeated text between chunks so we don't accidentally cut a sentence in half."
      ],
      "metadata": {
        "id": "QfF_mb_06BjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# 1. Configure the splitter\n",
        "# chunk_size=1000 means each piece will be roughly 1000 characters long\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "\n",
        "print(\"‚úÇÔ∏è Splitting the book into chunks...\")\n",
        "\n",
        "# 2. Split the documents\n",
        "# We are using the 'pages' variable from the previous step\n",
        "splits = text_splitter.split_documents(pages)\n",
        "\n",
        "print(f\"‚úÖ Done! We turned {len(pages)} pages into {len(splits)} small chunks.\")\n",
        "print(\"\\n--- Example of a single chunk ---\")\n",
        "print(splits[10].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8EoCBKRQyMw",
        "outputId": "bbb1f315-4b01-4f57-f914-46f1d6aca7e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÇÔ∏è Splitting the book into chunks...\n",
            "‚úÖ Done! We turned 348 pages into 1080 small chunks.\n",
            "\n",
            "--- Example of a single chunk ---\n",
            "5.1 Multilayer Perceptrons as ClassiÔ¨Åers ................................ 91\n",
            "5.2 Neural Network‚Äôs Error .............................................. 95\n",
            "5.3 Backpropagation of Error ............................................ 97\n",
            "5.4 Special Aspects of Multilayer Perceptrons .......................... 100\n",
            "5.5 Architectural Issues .................................................. 104\n",
            "5.6 Radial-Basis Function Networks .................................... 106\n",
            "5.7 Summary and Historical Remarks ................................... 109\n",
            "5.8 Solidify Your Knowledge ............................................ 110\n",
            "6 Decision Trees ............................................................... 113\n",
            "6.1 Decision Trees as ClassiÔ¨Åers ......................................... 113\n",
            "6.2 Induction of Decision Trees ......................................... 117\n",
            "6.3 How Much Information Does an Attribute Convey? ............... 119\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: The \"Translator\" (Embeddings)**\n",
        "We are going to turn those text chunks into **Lists of Numbers (called Vectors).**\n",
        "\n",
        "**Why?** Computers don't understand \"Machine Learning.\" They understand math.\n",
        "\n",
        "We will use a free model from Hugging Face to do this translation."
      ],
      "metadata": {
        "id": "p_Hp43gp6vDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "print(\"üîÆ Loading the Embedding Model (this translates text to numbers)...\")\n",
        "# We use a small, fast model called 'all-MiniLM-L6-v2'\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "print(\"üíæ Creating the Vector Database (this might take a moment)...\")\n",
        "# This takes your 1080 chunks, turns them into numbers, and saves them in memory\n",
        "vector_db = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=embedding_model\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Success! Your specific university brain is ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQslok8vRjWp",
        "outputId": "bedb430e-a58c-439d-ef29-989caae6fd7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÆ Loading the Embedding Model (this translates text to numbers)...\n",
            "üíæ Creating the Vector Database (this might take a moment)...\n",
            "‚úÖ Success! Your specific university brain is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To view the raw nubers(optional) run the following code"
      ],
      "metadata": {
        "id": "48HmNRek7Z31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Fetch one record from the database, including the raw math (embeddings)\n",
        "data = vector_db.get(limit=1, include=[\"embeddings\", \"documents\"])\n",
        "\n",
        "# 2. Extract the vector\n",
        "raw_vector = data['embeddings'][0]\n",
        "text_content = data['documents'][0]\n",
        "\n",
        "print(f\"üìÑ The Text Chunk starts with: '{text_content[:50]}...'\")\n",
        "print(f\"\\nüî¢ The Vector has {len(raw_vector)} dimensions (numbers).\")\n",
        "print(\"Here are the first 20 numbers of that vector:\")\n",
        "print(raw_vector[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ER3-LzD7bu2z",
        "outputId": "1d4f7e78-72e1-4a57-a2b1-3ea0faf8c1ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ The Text Chunk starts with: 'Miroslav/uni00A0Kubat\n",
            "An Introduction \n",
            "to Machine ...'\n",
            "\n",
            "üî¢ The Vector has 384 dimensions (numbers).\n",
            "Here are the first 20 numbers of that vector:\n",
            "[-0.03509036 -0.06561454  0.00587055 -0.02697201  0.01989697  0.04304998\n",
            "  0.02650404  0.02410588 -0.05778159 -0.04572349 -0.01883329 -0.01903909\n",
            " -0.00053616 -0.02233853 -0.05741227  0.06519616  0.01334015  0.00904386\n",
            " -0.02129865 -0.08295383]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4: The Search (Retrieval)**\n",
        "We will just ask your vector database to find the most relevant chunk for a specific question.\n",
        "\n",
        " **Here is how the math works:**\n",
        "\n",
        "1. You ask a question (e.g., \"What is a decision tree?\").\n",
        "\n",
        "2. The system turns your question into a vector (a list of numbers).\n",
        "\n",
        "3. It compares your question's vector to all 1080 chunk vectors in your database.\n",
        "\n",
        "4. It finds the ones that are mathematically closest (using something called Cosine Similarity)."
      ],
      "metadata": {
        "id": "QNTUFZC-7w5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define your question\n",
        "query = input(\"Enter your prompt \")\n",
        "\n",
        "print(f\"üïµÔ∏è‚Äç‚ôÄÔ∏è Searching for: '{query}'\")\n",
        "\n",
        "# 2. Ask the database to find the best matches (k=3 means find the top 3)\n",
        "results = vector_db.similarity_search(query, k=3)\n",
        "\n",
        "# 3. Print the results\n",
        "print(\"\\n--- Top Result Found ---\")\n",
        "print(results[0].page_content)\n",
        "\n",
        "print(\"\\n--- Source Metadata ---\")\n",
        "print(results[0].metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYs5aj2xffXk",
        "outputId": "df7a725d-5e4c-452e-fb67-2579d2a272e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your prompt what is machine learning\n",
            "üïµÔ∏è‚Äç‚ôÄÔ∏è Searching for: 'what is machine learning'\n",
            "\n",
            "--- Top Result Found ---\n",
            "Introduction\n",
            "Machine learning has come of age. And just in case you might think this is a mere\n",
            "platitude, let me clarify.\n",
            "The dream that machines would one day be able to learn is as old as computers\n",
            "themselves, perhaps older still. For a long time, however, it remained just that: a\n",
            "dream. True, Rosenblatt‚Äôs perceptron did trigger a wave of activity, but in retrospect,\n",
            "the excitement has to be deemed short-lived. As for the attempts that followed, these\n",
            "fared even worse; barely noticed, often ignored, they never made a breakthrough‚Äî\n",
            "no software companies, no major follow-up research, and not much support from\n",
            "funding agencies. Machine learning remained an underdog, condemned to live in\n",
            "the shadow of more successful disciplines. The grand ambition lay dormant.\n",
            "And then it all changed.\n",
            "A group of visionaries pointed out a weak spot in the knowledge-based systems\n",
            "that were all the rage in the 1970s‚Äô artiÔ¨Åcial intelligence: where was the ‚Äúknow-\n",
            "\n",
            "--- Source Metadata ---\n",
            "{'moddate': '2017-08-26T13:03:41+05:30', 'page': 9, 'source': 'An Introduction to Machine Learning -- Miroslav KubaÃÅt -- ( WeLib.org ).pdf', 'page_label': 'xi', 'producer': 'Adobe PDF Library 15.0', 'total_pages': 348, 'creationdate': '2017-08-26T12:39:54+05:30', 'creator': 'Adobe InDesign CC 2017 (Windows)'}\n"
          ]
        }
      ]
    }
  ]
}